{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent, RL and MultiEnvironment\n",
    "Try this notebook out interactively with: [![Binder](./img/badge_logo.svg)](https://mybinder.org/v2/gh/rte-france/Grid2Op/master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to have a look at the [00_Introduction](00_Introduction.ipynb), [01_Grid2opFramework](01_Grid2opFramework.ipynb) and [03_Action](03_Action.ipynb) and especially [04_TrainingAnAgent](04_TrainingAnAgent.ipynb) notebooks before getting into this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives**\n",
    "\n",
    "In this notebook we will introduce :\n",
    "* what is a \"SingleEnvMultiProcess\" (previously [MultiEnvironment](https://grid2op.readthedocs.io/en/v0.9.4/environment.html#grid2op.Environment.MultiEnvironment))\n",
    "* how can it be used with an agent\n",
    "* how can it be used to train a agent that uses different environments (using the [l2rpn-baselines](https://l2rpn-baselines.readthedocs.io/en/master/) python package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"200\">\n",
    "Execute the cell below by removing the # character if you use google colab !\n",
    "\n",
    "Cell will look like:\n",
    "```python\n",
    "!pip install grid2op[optional]  # for use with google colab (grid2Op is not installed by default)\n",
    "```\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install grid2op[optional]  # for use with google colab (grid2Op is not installed by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = None\n",
    "try:\n",
    "    from jyquickhelper import add_notebook_menu\n",
    "    res = add_notebook_menu()\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Impossible to automatically add a menu / table of content to this notebook.\\nYou can download \\\"jyquickhelper\\\" package with: \\n\\\"pip install jyquickhelper\\\"\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\anaconda3\\envs\\MA\\Lib\\site-packages\\grid2op\\Backend\\pandaPowerBackend.py:32: UserWarning: Numba cannot be loaded. You will gain possibly massive speed if installing it by \n",
      "\tc:\\Users\\thoma\\anaconda3\\envs\\MA\\python.exe -m pip install numba\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import grid2op\n",
    "TRAINING_STEP = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Make a regular environment and agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default we use the test environment. But by passing `test=False` in the following function will automatically download approximately 300MB from the internet and give you 1000 chronics instead of 2 used for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\anaconda3\\envs\\MA\\Lib\\site-packages\\grid2op\\MakeEnv\\Make.py:438: UserWarning: You are using a development environment. This environment is not intended for training agents. It might not be up to date and its primary use if for tests (hence the \"test=True\" you passed as argument). Use at your own risk.\n",
      "  warnings.warn(_MAKE_DEV_ENV_WARN)\n"
     ]
    }
   ],
   "source": [
    "env = grid2op.make(\"l2rpn_case14_sandbox\", test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of data have been made available for the default \"rte_case14_realistic\" environment. Including this data in the package is not convenient. \n",
    "\n",
    "We chose instead to release them and make them easily available with a utility. To download them in the default directory (\"~/data_grid2op/case14_redisp\")  just pass the argument \"test=False\" (or don't pass anything else) as local=False is the default value. It will download approximately 300Mo of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Train a standard RL Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you are using a computer with at least 4 cores if you want to notice some speed-ups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.Environment import SingleEnvMultiProcess\n",
    "from grid2op.Agent import DoNothingAgent\n",
    "NUM_CORE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIIa) Using the standard open AI gym loop\n",
    "\n",
    "Here we demonstrate how to use the multi environment class. First let's create a multi environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple agent\n",
    "agent = DoNothingAgent(env.action_space)\n",
    "\n",
    "# create the multi environment class\n",
    "multi_envs = SingleEnvMultiProcess(env=env, nb_env=NUM_CORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multi environment is just like using a regular environment but instead of dealing with one action, and one observation, it requires to be sent multiple actions, and returns a list of observations as well. \n",
    "\n",
    "It requires a grid2op environment to be initialized and creates some specific \"workers\", each is a replication of the initial environment. None of the \"worker\"s can be accessed directly. Supported methods are:\n",
    "- multi_env.reset\n",
    "- multi_env.step\n",
    "- multi_env.close\n",
    "\n",
    "These methods have similar behaviour as \"env.step\", \"env.close\" or \"env.reset\".\n",
    "\n",
    "\n",
    "It can be used the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<grid2op.Space.GridObjects.CompleteObservation_l2rpn_case14_sandbox object at 0x000001AE462987A0>,\n",
       "       <grid2op.Space.GridObjects.CompleteObservation_l2rpn_case14_sandbox object at 0x000001AE2268BFB0>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiliaze some variable with the proper dimension\n",
    "obss = multi_envs.reset()\n",
    "rews = [env.reward_range[0] for i in range(NUM_CORE)]\n",
    "dones = [False for i in range(NUM_CORE)]\n",
    "obss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, obs is not a single obervation, but a list (numpy nd array to be precise) of 4 observations (if 4 cores were used, otherwise the length of the array is the number of cores), each one being an observation of a given \"worker\" environment.\n",
    "\n",
    "Worker environments are always called in the same order. It means the first observation of this vector will always correspond to the first worker environment. \n",
    "\n",
    "\n",
    "Similarly to Observation, the \"step\" function of a multi_environment takes as input a list of multiple actions, each action will be implemented in its own environment. It returns a list of observations, a list of rewards, and boolean list of whether or not the worker environment suffers from a game over (in that case this worker environment is automatically restarted using the \"reset\" method.)\n",
    "\n",
    "Because orker environments are always called in the same order, the first action sent to the \"multi_env.step\" function will also be applied on this first environment.\n",
    "\n",
    "It is possible to use it as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<grid2op.Space.GridObjects.CompleteObservation_l2rpn_case14_sandbox object at 0x000001AE49C289E0>,\n",
       "       <grid2op.Space.GridObjects.CompleteObservation_l2rpn_case14_sandbox object at 0x000001AE49C28950>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the vector of actions that will be processed by each worker environment.\n",
    "acts = [None for _ in range(NUM_CORE)]\n",
    "for env_act_id in range(NUM_CORE):\n",
    "    acts[env_act_id] = agent.act(obss[env_act_id], rews[env_act_id], dones[env_act_id])\n",
    "    \n",
    "# feed them to the multi_env\n",
    "obss, rews, dones, infos = multi_envs.step(acts)\n",
    "\n",
    "# as explained, this is a vector of Observation (as many as NUM_CORE in this example)\n",
    "obss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi environment loop is really close to the \"gym\" loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs the appropriated steps\n",
    "for i in range(10):\n",
    "    acts = [None for _ in range(NUM_CORE)]\n",
    "    for env_act_id in range(NUM_CORE):\n",
    "        acts[env_act_id] = agent.act(obss[env_act_id], rews[env_act_id], dones[env_act_id])\n",
    "    obss, rews, dones, infos = multi_envs.step(acts)\n",
    "\n",
    "    # DO SOMETHING WITH THE AGENT IF YOU WANT\n",
    "    ## agent.train(obss, rews, dones)\n",
    "    \n",
    "\n",
    "# close the environments created by the multi_env\n",
    "multi_envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the above example, `TRAINING_STEP` steps are performed on `NUM_CORE` environments in parrallel. The agent has then acted `TRAINING_STEP * NUM_CORE` (=`10 * 4 = 40` by default) times on `NUM_CORE` different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.b) Practical example (work in progress)\n",
    "\n",
    "We reuse the code of the Notebook [04_TrainingAnAgent](04_TrainingAnAgent.ipynb) to train a new agent we strongly recommend you to have a look at it if it is not done already.\n",
    "\n",
    "In this notebook, we focus on how to make your agent interact with multiple environments at the same time (**eg** it means that the batch of data the agent receives comes from different instance of the same environment, this is a technique used in [Asynchronous Actor Critic - A3C](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2) type of models for example).\n",
    "\n",
    "You will see with this method you can learn the same baseline with minimal (actually without any) changes in the above way.\n",
    "\n",
    "#### Note on the implementation\n",
    "The most common code for A3C agent can be summarized in the image below (image credit [this blog post](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2)):\n",
    "![](https://miro.medium.com/max/1400/1*YtnGhtSAMnnHSL8PvS7t_w.png)\n",
    "\n",
    "In this image you see different version of the agent that interacts with different versions of the (same) environment (represented here in the different *workers*). And, from time to time, each \"worker\" will send its weights to the \"global network\" and an update procedure will be run there. Though this framework can perfectly be implemented in grid2op, the *SingleEnvMultiProcess* class works a bit differently.\n",
    "\n",
    "Actually, in this *SingleEnvMultiProcess* class, there exists only one copy of the \"global network\" that is kept into the main \"thread\" (which to be precise is a process) and that interacts (send actions to and receives observations from) with different independant environments. It is really similar to the [SubprocVecEnv](https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/vec_env/subproc_vec_env.py#L39) of open ai gym.\n",
    "\n",
    "This is especially suited in the case of powersystem operations, as it can be quite computationnally expensive to solve for the powerflow equations at each nodes of the grid (also called [Kirchhoff's laws](https://en.wikipedia.org/wiki/Kirchhoff%27s_circuit_laws))\n",
    "\n",
    "Here, you will see in the next cell how to (*not really*) change it to train a agent on different environments:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
